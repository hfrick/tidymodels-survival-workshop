---
title: "5 - Tuning models"
subtitle: "Survival analysis with tidymodels"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
    fig.path: "figures/"
---

```{r}
#| label: setup
#| include: false
#| file: setup.R
```

```{r setup-previous}
#| echo: false
library(tidymodels)
library(censored)
library(splines)

data(cat_adoption)

cat_adoption <- 
  cat_adoption %>% 
  mutate(event_time = Surv(time, event), .keep = "unused", .before = everything()) 

set.seed(27)
in_demo <- sample.int(nrow(cat_adoption), 50)
demo_cats <- cat_adoption %>% slice(in_demo)

set.seed(123)
cat_split <- initial_split(cat_adoption %>% slice(-in_demo), prop = 0.8)
cat_train <- training(cat_split)
cat_test <- testing(cat_split)

set.seed(123)
cat_folds <- vfold_cv(cat_train, v = 10)
```

## Tuning parameters

Some model or preprocessing parameters cannot be estimated directly from the data.

. . .

Some examples:

- Tree depth in decision trees
- Number of neighbors in a K-nearest neighbor model

## Optimize tuning parameters

- Try different values and measure their performance.

. . .

- Find good values for these parameters.

. . .

- Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.

## Optimize tuning parameters

The main two strategies for optimization are:

. . .

-   **Grid search** ðŸ’  which tests a pre-defined set of candidate values

-   **Iterative search** ðŸŒ€ which suggests/estimates new values of candidate parameters to evaluate

## Specifying tuning parameters

Let's take our previous random forest workflow and tag for tuning the minimum number of data points in each node:

```{r}
#| label: tag-for-tuning
#| code-line-numbers: "1|"

rf_spec <- rand_forest(min_n = tune()) %>% 
  set_engine("aorsf") %>% 
  set_mode("censored regression")

rf_wflow <- workflow(event_time ~ ., rf_spec)
rf_wflow
```

## Try out multiple values

`tune_grid()` works similar to `fit_resamples()` but covers multiple parameter values:

```{r}
#| label: rf-tune_grid
#| code-line-numbers: "2|3-5|6|"

set.seed(22)
rf_res <- tune_grid(
  rf_wflow,
  cat_folds,
  eval_time = c(90, 30, 60, 120),
  grid = 5
)
```

## Compare results

Inspecting results and selecting the best-performing hyperparameter(s):

```{r}
#| label: rf-results

show_best(rf_res)

best_parameter <- select_best(rf_res)
best_parameter
```

`collect_metrics()` and `autoplot()` are also available.

## The final fit

```{r}
#| label: rf-finalize

rf_wflow <- finalize_workflow(rf_wflow, best_parameter)

final_fit <- last_fit(rf_wflow, cat_split, eval_time = c(90, 30, 60, 120)) 

collect_metrics(final_fit)
```

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Modify your model workflow to tune one or more parameters.*

*Use grid search to find the best parameter(s).*

```{r ex-tune-grid}
#| echo: false
countdown::countdown(minutes = 5, id = "tune-grid")
```
