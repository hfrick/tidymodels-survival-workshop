---
title: "4 - Evaluating models"
subtitle: "Introduction to tidymodels"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
    fig.path: "figures/"
---

```{r setup}
#| include: false
#| file: setup.R
```

## Previously

```{r}
#| label: pref-previous
library(tidymodels)
library(censored)

cat_adoption <- 
  cat_adoption %>% 
  mutate(event_time = Surv(time, event), .keep = "unused", .before = everything()) 

set.seed(1)
cat_split <- initial_split(cat_adoption)
cat_train <- training(cat_split)
cat_test <- training(cat_split)
```

## An Example Model Fit

```{r}
#| label: example-fit
tree_fit <- 
  decision_tree(mode = "censored regression") %>%
  set_engine("partykit") %>% 
  fit(event_time ~ ., data = cat_train)

set.seed(12)
franken_cats <- map_dfc(cat_test, ~ sample(.x, 500))
```


## Types of Predictions

```{r}
#| label: franken-cat-preds
franken_cat_preds <- augment(tree_fit, franken_cats, eval_time = 1:360)
franken_cat_preds %>% select(1:3)
franken_cat_preds$.pred[[1]]
```

## Survival Probabilities


```{r}
#| label: franked-probs

franken_cat_preds %>% 
  slice(1:4) %>% 
  add_rowindex() %>% 
  mutate(cat = paste0(.row, " (", format(event_time), ")")) %>% 
  unnest(.pred) %>% 
  ggplot(aes(.eval_time, .pred_survival, group = cat, col = cat)) + 
  geom_step() +
  labs(x = "Evaluation Time", y = "Pr[Not Adopted Yet]")

```

## Concordance

The concordance statistic (“c-index”) is a metric that quantifies that the rank order of the times is consistent with some model score (e.g., a survival probability). 

It takes into account censoring and does not depend on a specific evaluation time. 

It has some weaknesses (see ref and ref) and is used less than it originally was. 

## Concordance

```{r}
#| label: concordance
franken_cat_preds %>% 
  concordance_survival(event_time, estimate = .pred_time)
```

The other metrics that we will discuss are dynamic in the sense that they evaluate the model at specific time points (“evaluation times”). 


## Time-dependent metrics

show some survival probabilities

## Classification(ish) Metrics

Most dynamic metrics convert the survival probabilities to events and non-events based on some probability threshold. 

From there, we can apply existing classification metrics, such as

- Brier Score (for calibration)
- Area under the ROC curve (for separation)

We’ll talk about both of these. 

First, how do we handle cases where the observed time is censored before the evaluation time? 

## Converting to Events

For a specific time point $\tau$, we convert the observed event time to a binary event/non-event version (if possible) ($y_{i\tau} \in \{0, 1\}$). 

$$
y_{i\tau} = 
\begin{cases}
1 & \text{if } t_{i} \leq \tau\text{ and  event} \notag \\ 
0 & \text{if } t_{i} \gt \tau \text{ and } either \notag \\ 
N/A & \text{if } t_{i} \leq \tau\text{ and censored }
\end{cases}
$$

The predicted class probabilities are then: 

$$
\begin{align}
Pr[y_{i\tau} = 1] &= 1- \hat{S}_i(\tau; x_{i})\notag \\
Pr[y_{i\tau} = 0] &= \hat{S}_i(\tau; x_{i}) \notag 
\end{align}
$$




(mention $\epsilon$)

## Strong Opinions

There are _a lot_ of papers with different methods of evaluating survival data with dynamic metrics. Almost all of these are created to help score/screen predictors. 

The problem is that these metrics are not derived for estimated model probabilities. Statistically, this is a very different problem. 

Our (rather strong) opinion is that most methods are not applicable for evaluating predictive event time models. 

tidymodels uses an approach similar to causal inference called _inverse probability of censoring weights_ (IPCW). We feel that this is the best approach that we know of. 

## Inverse Probability Weighting

## Probability of Censoring

$\hat{C}(T;x_i)$ is the probability that sample $i$ is censored at some generic time $T$. 

$$
Pr[censored] = 
\begin{cases}
\hat{C}(t_i;x_i) & \text{if } y_{i\tau} = 1 \\ \notag
\hat{C}(\tau;x_i)  & \text{if } y_{i\tau} = 0  \notag 
\end{cases}
$$
(note the time of censoring)


$w_i$ is the inverse of this probability. 

## Probability of Censoring

We currently only estimate the probability of non-informative censoring (i.e. the predictors $x_i$ are ignored). 

 - Our estimator $\hat{C}(T;x_i)$ is the “reverse Kaplan-Meier” (RKM) curve that inverts the event indicator. 
 - This is the same for each sample at time $\tau$. 

This curve is attached to the parsnip model object. For our data set...

## Reverse Kaplan-Meier Curve

```{r}
#| label: reverse-km
#| echo: false
#| fig-width: 6
#| fig-height: 4
#| out-width: 80%
#| fig-align: center
rkm_data <- 
  tree_fit$censor_probs$fit[c("time", "surv")] %>% 
  as_tibble() %>% 
  set_names(c(".eval_time", ".pred_censored"))

franken_cat_preds %>% 
  slice(1:4) %>% 
  add_rowindex() %>% 
  mutate(cat = paste0("cat ", .row, " (", format(event_time), ")")) %>% 
  unnest(.pred) %>% 
  ggplot(aes(.weight_time, .pred_censored)) + 
  geom_step(data = rkm_data, aes(x = .eval_time), alpha = 1 / 5, linewidth = 1 / 2) +
  geom_step(aes(group = cat, col = cat), show.legend = FALSE, linewidth = 1) +
  labs(x = "Evaluation Time", y = "Prob Cesnored") + 
  lims(y = 0:1) +
  facet_wrap(~ cat)
```


## Inverse Weights

```{r}
#| label: ipcw
#| echo: false
#| fig-width: 6
#| fig-height: 4
#| out-width: 80%
#| fig-align: center
max_wt_time <- 
  franken_cat_preds %>% 
  slice(1:4) %>% 
  add_rowindex() %>% 
  mutate(cat = paste0("cat ", .row, " (", format(event_time), ")")) %>%   
  unnest(.pred) %>% 
  summarize(max_weight_time = max(.weight_time, na.rm = TRUE), .by = c(cat))

franken_cat_preds %>% 
  slice(1:4) %>% 
  add_rowindex() %>% 
  mutate(cat = paste0("cat ", .row, " (", format(event_time), ")")) %>% 
  unnest(.pred) %>% 
  ggplot(aes(.eval_time, .weight_censored)) + 
  geom_step(aes(group = cat, col = cat), show.legend = FALSE) +
  geom_vline(data = max_wt_time, aes(xintercept = max_weight_time, col = cat), 
             lty = 3, show.legend = FALSE) +
  labs(x = "Evaluation Time", y = "Prob Cesnored") + 
  facet_wrap(~ cat)
```

## Brier Score

The Brier score is a measure of calibration originally meant for classification models. 





$$
Brier = \frac{1}{N}\sum_{i=1}^Nw_i\left[\hat{S}_i(\tau; x_{i})^2 + \left(y_{i\tau} - \hat{S}_i(\tau; x_{i})\right)^2 \right]
$$
