---
title: "4 - Evaluating models"
subtitle: "Survival analysis with tidymodels"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
    fig.path: "figures/"
---

```{r setup}
#| include: false
#| file: setup.R
```

```{r}
#| label: setup-previous
#| echo: false
library(tidymodels)
library(censored)
library(splines)

data(cat_adoption)

cat_adoption <- 
  cat_adoption %>% 
  mutate(event_time = Surv(time, event), .keep = "unused", .before = everything()) 

set.seed(27)
in_demo <- sample.int(nrow(cat_adoption), 50)
demo_cats <- cat_adoption %>% slice(in_demo)

set.seed(123)
cat_split <- initial_split(cat_adoption %>% slice(-in_demo), prop = 0.8)
cat_train <- training(cat_split)
cat_test <- testing(cat_split)
```

## An Example Model Fit {.annotation} 

Let's fit another PH and add some nonlinear terms via [splines](https://aml4td.org/chapters/interactions-nonlinear.html#sec-splines):

```{r}
#| label: example-fit
# First add all of the predictors...
f <- event_time ~ . -
  # Then remove geocoded columns
  longitude - latitude +
  # Then add them back as spline terms
  ns(longitude, df = 5) + ns(latitude, df = 5)

cat_wflow <- workflow(f, proportional_hazards())

cph_spline_fit <- cat_wflow %>%
  fit(data = cat_train)
```

## Predictions

For our demo data set of n = `r nrow(demo_cats)` cats, the largest event time was `r max(demo_cats$event_time[,1][demo_cats$event_time[,2] ==1])`. 

We'll make predictions from 8 days to 320 days: 

```{r}
#| label: demo-cat-preds
demo_cat_preds <- augment(cph_spline_fit, demo_cats, eval_time = 8:320)
demo_cat_preds %>% select(1:3)
```
```{r}
#| label: save-pred-for-annotations
#| include: false
cph_spline_fit_parsnip <- extract_fit_parsnip(cph_spline_fit)
rkm_data <- 
  cph_spline_fit_parsnip$censor_probs$fit[c("time", "surv")] %>% 
  as_tibble() %>% 
  set_names(c(".eval_time", ".pred_censored"))

save(demo_cat_preds, file = "RData/demo_cat_preds.RData")
save(rkm_data, file = "RData/rkm_data.RData")
```

## Concordance

The concordance statistic (“c-index”, [Harrell _et al_ (1996)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22multivariable+prognostic+models%3A+issues+in+developing+models%2C+evaluating+assumptions+and+adequacy%2C+and+measuring+and+reducing+errors%22&btnG=)) is a metric that quantifies that the rank order of the times is consistent with some model score (e.g., a survival probability). 

<br> 

It takes into account censoring and does not depend on a specific evaluation time. The range of values is $[-1, 1]$. 

```{r}
#| label: concordance
demo_cat_preds %>% 
  concordance_survival(event_time, estimate = .pred_time)
```

## Time-dependent metrics

We pick specific time points to evaluate the model (depending on our problem) such as every 30 days: 

```{r}
#| label: demo-probs-again
#| echo: false
#| fig-width: 6
#| fig-height: 4
#| out-width: 80%
#| fig-align: center
demo_cat_preds %>% 
  slice(1:4) %>% 
  add_rowindex() %>% 
  mutate(cat = paste0("#", .row, " (", format(event_time), ")")) %>% 
  unnest(.pred) %>% 
  ggplot(aes(.eval_time, .pred_survival, group = cat, col = cat)) + 
  geom_line() +
  geom_vline(xintercept = (1:12) * 30, col = "black", lty = 3, alpha = 7 / 10) +
  labs(x = "Evaluation time", y = "Probability of not adopted")
```


## Classification(ish) Metrics

Most dynamic metrics convert the survival probabilities to events and non-events based on some probability threshold. 

From there, we can apply existing classification metrics, such as

- Brier Score (for calibration)
- Area under the ROC curve (for separation)

We’ll talk about both of these. 

There are more details on dynamics metrics at [tidymodels.org](https://www.tidymodels.org/learn/#category=survival%20analysis). 

## Converting to Events {.annotation} 

For a specific time point $\tau$, we convert the observed event time to a binary event/non-event version (if possible) ($y_{i\tau} \in \{0, 1\}$). 

$$
y_{i\tau} = 
\begin{cases}
1 & \text{if } t_{i} \leq \tau\text{ and  event} \notag \\ 
0 & \text{if } t_{i} \gt \tau \text{ and } either \notag \\ 
missing & \text{if } t_{i} \leq \tau\text{ and censored }
\end{cases}
$$

## Converting to Events

```{r}
#| label: plot-graf-categories
#| echo: false
#| warning: false
#| fig-width: 8
#| fig-height: 4
#| out-width: 70%
#| fig-align: center
obs_time <- c(4, 2)
obs_status <- c("censored", "event")

df1 <- tibble::tibble(
  obs_id = 1:2,
  obs_time = obs_time,
  obs_status = obs_status,
  eval_time = 1,
  eval_status = c("censored", "censored")
)
df2 <- tibble::tibble(
  obs_id = 1:2,
  obs_time = obs_time,
  obs_status = obs_status,
  eval_time = 3,
  eval_status = c("censored", "event")
)
df3 <- tibble::tibble(
  obs_id = 1:2,
  obs_time = obs_time,
  obs_status = obs_status,
  eval_time = 5,
  eval_status = c(NA, "event")
)
df <- bind_rows(df1, df2, df3)

pch_dot_empty <- 1
pch_dot_solid <- 19
pch_triangle_empty <- 2
pch_triangle_solid <- 17

df %>% 
  dplyr::mutate(
    obs_status = dplyr::if_else(obs_status == "censored", pch_dot_empty, pch_dot_solid),
    eval_status = dplyr::if_else(eval_status == "censored", pch_triangle_empty, pch_triangle_solid)
  ) %>% 
  ggplot() +
  geom_point(aes(obs_time, obs_id, shape = obs_status, size = I(5))) +
  geom_segment(aes(x = rep(0, 6), y = obs_id, xend = obs_time, yend = obs_id)) +
  geom_vline(aes(xintercept = eval_time, col = I("red"), linetype = I("dashed"), linewidth = I(0.8))) +
  geom_point(aes(eval_time, obs_id, shape = eval_status, col = I("red"), size = I(5))) +
  scale_shape_identity("Status",
                       labels = c("Observation: censored", "Observation: event",
                                  "Evaluation: non-event", "Evaluation: event"),
                       breaks = c(1, 19, 2, 17),
                       guide = "legend") +
  scale_x_continuous(limits = c(0, 7)) +
  scale_y_continuous(limits = c(0.5, 2.5)) +
  labs(x = "Time", y = "Sample") +
  theme_bw() +
  theme(axis.text.y = element_blank(), legend.position = "top") +
  facet_grid(~ eval_time) 
```

## Dealing with Missing Outcome Data {.annotation} 

Without censored data points, this conversion would yield appropriate performance estimates since no event outcomes would be missing. 

<br>

Otherwise, there is the potential for bias due to missingness. 

<br>

We'll use tools from causal inference to compensate by creating a propensity score that uses the probability of being censored. 

Case weights use the inverse of this probability. See [Graf _et al_ (1999)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=graf+1999+%22Assessment+and+comparison+of+prognostic+classification+schemes+for+survival+data%22&btnG=).

## Brier Score

The Brier score is calibration metric originally meant for classification models:

$$
Brier = \frac{1}{N}\sum_{i=1}^N\sum_{k=1}^C (y_{ik} - \hat{\pi}_{ik})^2
$$

For our application, we have two classes and case weights

:::{style="overflow-x:auto;overflow-y:hidden;"}

$$
Brier(t) = \frac{1}{W}\sum_{i=1}^N w_{it}\left[\underbrace{I(y_{it} = 0)(y_{it} - \hat{p}_{it})^2}_\text{non-events} +  \underbrace{I(y_{it} = 1)(y_{it} - (1 - \hat{p}_{it}))^2}_\text{events}\right]
$$

:::


## Brier Scores

```{r}
#| label: brier-ex
demo_brier <- brier_survival(demo_cat_preds, truth = event_time, .pred)
demo_brier %>% filter(.eval_time %in% seq(30, 300, by = 30))
```

## Brier Scores Over Evaluation Time

```{r}
#| label: brier-time
#| echo: false
#| fig-width: 6
#| fig-height: 4.25
#| out-width: 60%
#| fig-align: center
demo_brier %>% 
  ggplot(aes(.eval_time, .estimate)) + 
  geom_line() + 
  geom_hline(yintercept = 0, col = "green")+
  labs(x = "Evaluation time", y = "Brier score")
```

## Integrated Brier Score

```{r}
#| label: brier-int-ex
brier_survival_integrated(demo_cat_preds, truth = event_time, .pred)
```

## Area Under the ROC Curve

This is more straightforward. 

<br>

We can use the standard ROC curve machinery once we have the indicators, probabilities, and censoring weights at time $\tau$ ([Hung and Chiang (2010)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22Optimal+Composite+Markers+for+Time-Dependent+Receiver+Operating+Characteristic+Curves+with+Censored+Survival+Data%22&btnG=)). 

<br>

ROC curves measure the separation between events and non-events and are ignorant of how well-calibrated the probabilities are.  


## Area Under the ROC Curve 

```{r auc-survival}
demo_row_auc <- roc_auc_survival(demo_cat_preds, truth = event_time, .pred)
demo_row_auc %>% filter(.eval_time %in% seq(30, 300, by = 30))
```

## ROC AUC Over Evaluation Time

```{r}
#| label: auc-time
#| echo: false
#| fig-width: 6
#| fig-height: 4.25
#| out-width: 60%
#| fig-align: center
demo_row_auc %>% 
  ggplot(aes(.eval_time, .estimate)) + 
  geom_line() + 
  geom_hline(yintercept = 1, col = "green")+
  geom_hline(yintercept = 1/2, col = "red")+
  labs(x = "Evaluation time", y = "ROC AUC")
```

## Using Evaluation Times

When predicting, you can get predictions at any values of $\tau$. 

<br>

During model development, we suggest picking a more focused set of evaluation times (for computational time). 

<br>

You should also pick a time to perform your optimizations/comparisons and list that value first in the vector. If 90 days was of interest, you might use

```{r}
#| label: time-example
times <- c(90, 30, 60, 120)
```



# ⚠️ DANGERS OF OVERFITTING ⚠️

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Why haven't we done this?*

```{r augment-acc-2}
#| eval: false
cph_spline_fit %>%
  augment(cat_train, eval_time = 8:320) %>%
  brier_survival(truth = event_time, .pred)
```

```{r ex-overfitting}
#| echo: false
countdown::countdown(minutes = 5, id = "overfitting")
```

::: {.notes}
- repredicting the training set is overly optimistic
- optimising that leads to overfitting
:::

# The testing data are precious 💎

# How can we use the *training* data to compare and evaluate different models? 🤔

##  {background-color="white" background-image="https://www.tmwr.org/premade/resampling.svg" background-size="70%"}

## Cross-validation `r hexes("rsample")` {.annotation} 

```{r vfold-cv}
set.seed(123)
cat_folds <- vfold_cv(cat_train, v = 10)# v = 10 is default
cat_folds
```

## Cross-validation `r hexes("rsample")`

What is in this?

```{r cat-splits}
cat_folds <- vfold_cv(cat_train)
cat_folds$splits[1:3]
```

. . .

Set the seed when creating resamples

::: notes
Talk about a list column, storing non-atomic types in dataframe
:::

## Bootstrapping `r hexes("rsample")` {.annotation} 

```{r bootstraps}
set.seed(3214)
bootstraps(cat_train)
```

##  {background-iframe="https://rsample.tidymodels.org/reference/index.html"}

::: footer
:::

## The whole game - status update

```{r diagram-resamples, echo = FALSE}
#| fig-align: "center"

knitr::include_graphics("images/whole-game-transparent-resamples.jpg")
```

# We are equipped with metrics and resamples!

## Fit our model to the resamples

```{r fit-resamples}
cat_res <- fit_resamples(cat_wflow, cat_folds, eval_time = c(90, 30, 60, 120))
cat_res
```

## Evaluating model performance `r hexes("tune")`

```{r collect-metrics}
cat_res %>%
  collect_metrics()
```

::: notes
`collect_metrics()` is one of a suite of `collect_*()` functions that can be used to work with columns of tuning results. Most columns in a tuning result prefixed with `.` have a corresponding `collect_*()` function with options for common summaries.
:::

. . .

We can reliably measure performance using only the **training** data 🎉

## Where are the fitted models? `r hexes("tune")`  {.annotation}

```{r cat-res}
cat_res
```

. . .

🗑️

## But it's easy to save the predictions `r hexes("tune")`

```{r save-predictions}
# Save the assessment set results
ctrl_cat <- control_resamples(save_pred = TRUE)
cat_res <- fit_resamples(cat_wflow, cat_folds, eval_time = c(90, 30, 60, 120),
                         control = ctrl_cat)

cat_res
```

## But it's easy to collect the predictions `r hexes("tune")`

```{r collect-predictions}
cat_preds <- collect_predictions(cat_res)
cat_preds
```

# Decision tree 🌳

# Random forest 🌳🌲🌴🌵🌴🌳🌳🌴🌲🌵🌴🌲🌳🌴🌳🌵🌵🌴🌲🌲🌳🌴🌳🌴🌲🌴🌵🌴🌲🌴🌵🌲🌵🌴🌲🌳🌴🌵🌳🌴🌳

## Random forest 🌳🌲🌴🌵🌳🌳🌴🌲🌵🌴🌳🌵

- Ensemble many decision tree models

- All the trees vote! 🗳️

- Bootstrap aggregating + random predictor sampling

. . .

- Often works well without tuning hyperparameters (more on this in a moment), as long as there are enough trees

## Create a random forest model `r hexes("parsnip")`

```{r rf-spec}
rf_spec <- rand_forest(trees = 1000) %>% 
  set_engine("aorsf") %>% 
  set_mode("censored regression")
rf_spec
```

## Create a random forest model `r hexes("workflows")`

```{r rf-wflow}
rf_wflow <- workflow(event_time ~ ., rf_spec)
rf_wflow
```

## Your turn {transition="slide-in"}

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Use `fit_resamples()` and `rf_wflow` to:*

-   *keep predictions*
-   *compute metrics*

```{r ex-try-fit-resamples}
#| echo: false
countdown::countdown(minutes = 8, id = "try-fit-resamples")
```

## Evaluating model performance `r hexes("tune")`

```{r collect-metrics-rf}
ctrl_cat <- control_resamples(save_pred = TRUE)

# Random forest uses random numbers so set the seed first
set.seed(2)
rf_res <- fit_resamples(rf_wflow, cat_folds, eval_time = c(90, 30, 60, 120), 
                        control = ctrl_cat)

collect_metrics(rf_res)
```

## The whole game - status update

```{r diagram-select, echo = FALSE}
#| fig-align: "center"

knitr::include_graphics("images/whole-game-transparent-select.jpg")
```

## The final fit `r hexes("tune")` 

Suppose that we are happy with our random forest model.

Let's fit the model on the training set and verify our performance using the test set.

. . .

We've shown you `fit()` and `predict()` (+ `augment()`) but there is a shortcut:

```{r final-fit}
# cat_split has train + test info
final_fit <- last_fit(rf_wflow, cat_split, eval_time = c(90, 30, 60, 120)) 

final_fit
```

## What is in `final_fit`? `r hexes("tune")`

```{r collect-metrics-final-fit}
collect_metrics(final_fit)
```

. . .

These are metrics computed with the **test** set

## What is in `final_fit`? `r hexes("tune")`

```{r extract-workflow}
extract_workflow(final_fit)
```

. . .

Use this for **prediction** on new data, like for deploying

## The whole game

```{r diagram-final-performance, echo = FALSE}
#| fig-align: "center"

knitr::include_graphics("images/whole-game-final-performance.jpg")
```
