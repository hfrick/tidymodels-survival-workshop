---
title: "Annotations"
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
    fig.path: "figures/"
---

```{r startup}
#| include: false

library(tidymodels)
library(censored)
tidymodels_prefer()

# To get `hexes()`
source("setup.R")
```

<hr size="5">

# 01 - Introduction

## `r emo::ji("eyes")`

This page contains _annotations_ for selected slides. 

There's a lot that we want to tell you. We don't want people to have to frantically scribble down things that we say that are not on the slides. 

We've added sections to this document with longer explanations and links to other resources. 

<hr size="5">

# 02 - Data Budget

## The initial split

What does `set.seed()` do? 

We’ll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random). 

Think of PRN as a box that takes a starting value (the "seed") that produces random numbers using that starting value as an input into its process. 

If we know a seed value, we can reproduce our "random" numbers. To use a different set of random numbers, choose a different seed value. 

For example: 

```{r}
set.seed(1)
runif(3)

# Get a new set of random numbers:
set.seed(2)
runif(3)

# We can reproduce the old ones with the same seed
set.seed(1)
runif(3)
```

If we _don’t_ set the seed, R uses the clock time and the process ID to create a seed. This isn’t reproducible. 

Since we want our code to be reproducible, we set the seeds before random numbers are used. 

In theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same and we don’t get reproducible results. 

The value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to "spread the randomness around". It is basically:

```{r}
cat(paste0("set.seed(", sample.int(10000, 5), ")", collapse = "\n"))
```

<hr size="5">

# 03 - What Makes A Model?

## What is wrong with this? 

If we treat the preprocessing as a separate task, it raises the risk that we might accidentally overfit to the data at hand. 

For example, someone might estimate something from the entire data set (such as the principle components) and treat that data as if it were known (and not estimated). Depending on the what was done with the data, consequences in doing that could be:

* Your performance metrics are slightly-to-moderately optimistic (e.g. you might think your accuracy is 85% when it is actually 75%)
* A consequential component of the analysis is not right and the model just doesn’t work. 

The big issue here is that you won’t be able to figure this out until you get a new piece of data, such as the test set. 

A really good example of this is in [‘Selection bias in gene extraction on the basis of microarray gene-expression data’](https://pubmed.ncbi.nlm.nih.gov/11983868/). The authors re-analyze a previous publication and show that the original researchers did not include feature selection in the workflow. Because of that, their performance statistics were extremely optimistic. In one case, they could do the original analysis on complete noise and still achieve zero errors. 

Generally speaking, this problem is referred to as [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)). Some other references: 

 * [Overfitting to Predictors and External Validation](https://bookdown.org/max/FES/selection-overfitting.html)
 * [Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/757b505cfd34c64c85ca5b5690ee5293-Abstract-round2.html)
 * [Navigating the pitfalls of applying machine learning in genomics](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Navigating+the+pitfalls+of+applying+machine+learning+in+genomics&btnG=)
 * [A review of feature selection techniques in bioinformatics](https://academic.oup.com/bioinformatics/article/23/19/2507/185254)
 * [On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation](https://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf)

<hr size="5">

# 04 - Evaluating Models

## An Example Model Fit

In regards to the weird formula assembly: this will be so much easier once we introduce recipes. 

## Converting to Events

The predicted class probabilities are then: 

$$
\begin{align}
Pr[y_{i\tau} = 1] &= 1- \hat{S}(\tau; \boldsymbol{x}_{i})\notag \\
Pr[y_{i\tau} = 0] &= \hat{S}(\tau; \boldsymbol{x}_{i}) \notag 
\end{align}
$$


## Dealing with Missing Outcome Data

For our causal inference approach, we need to compute $\hat{C}(t^*;\boldsymbol{x}_{i})$,  the probability that sample $i$ is censored at some time $t^*$. 

We have to consider how to compute the time point $t^*$. Let’s say we are predicting what will happen at $t^* = 17$ days. 

First, we are predicting future events, so we can only assume that we have data _prior to_ 17 days. For this reason, when we use the observed time to compute the probability of censoring, we do it just prior to $t^*$, at $t_i - \epsilon$ for some very small $\epsilon$.

Second, following [Graf _et al._ (1999)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=graf+1999+%22Assessment+and+comparison+of+prognostic+classification+schemes+for+survival+data%22&btnG=), we calculate the censoring probability differently for each event class:  

$$
t_i^*= 
\begin{cases}
t_i  - \epsilon &  \text{if }t_i \le \tau \\ \notag
\tau - \epsilon &  \text{if }t_i > \tau  \notag 
\end{cases}
$$

How exactly do we estimate $\hat{C}(t^*;\boldsymbol{x}_{i})$? We currently only estimate the probability of non-informative right censoring (i.e., the predictors $x_i$ are ignored). We may expand this API in the future when you have informative censoring.  

Our estimator $\hat{C}(T;x_i)$ is the “reverse Kaplan-Meier” (RKM, [Korn (1986)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=Korn+1986+%22Censoring+distributions+as+a+measure+of+follow-up+in+survival+analysis%22&btnG=)) curve that inverts the event indicator. 

The RKM curve is attached to the parsnip model object. Its curve: 

```{r}
#| label: reverse-km
#| echo: false
#| warning: false
#| fig-width: 6
#| fig-height: 4
#| out-width: 80%
#| fig-align: center
load(file = "RData/demo_cat_preds.RData")
load(file = "RData/rkm_data.RData")

max_wt_time <- 
  demo_cat_preds %>% 
  slice(1:4) %>% 
  add_rowindex() %>% 
  mutate(cat = paste0("#", .row, " (", format(event_time), ")")) %>% 
  unnest(.pred) %>% 
  summarize(max_weight_time = max(.weight_time, na.rm = TRUE), .by = c(cat))

rkm_data %>% 
  ggplot(aes(.eval_time, .pred_censored)) +
  geom_line(alpha = 1 / 5, linewidth = 2) +
  labs(x = "Evaluation Time", y = "Prob Censored") + 
  lims(y = 0:1)
```

Now let's look at how the probabilities are computed for four specific cats: 

```{r}
#| label: weight-examples
#| echo: false
#| warning: false
#| fig-width: 6
#| fig-height: 4
#| out-width: 80%
#| fig-align: center

demo_cat_preds %>% 
  slice(1:4) %>% 
  add_rowindex() %>% 
  mutate(cat = paste0("#", .row, " (", format(event_time), ")")) %>% 
  unnest(.pred) %>% 
  filter(.eval_time <= 50) %>% 
  ggplot(aes(.eval_time, .pred_censored)) + 
  geom_line(aes(group = cat, col = cat), show.legend = FALSE) +
  geom_vline(xintercept = 10, col = "black", lty = 2) +
  geom_vline(xintercept = 30, col = "black", lty = 3) +
  labs(x = "Evaluation Time", y = "Prob Censored") + 
  lims(y = 0:1) +
  facet_wrap(~ cat)
```

We've truncated the x-axis for readability. 

From this: 

* At $\tau = 10$, all four cats are used to measure performance and $t^*_i = t_i - \epsilon$. 

* At $\tau = 30$, only two cats are used and $t^*_1 = 36 - \epsilon$ and $t^*_4 = 30 - \epsilon$.

How the censoring probability varies over time can greatly impact the metrics. For the demonstration set, there are very few usable outcomes in the late stages of the analysis, but these have large weights. 

```{r}
#| label: weight-sum
#| echo: false
#| fig-width: 6
#| fig-height: 4.25
#| out-width: 80%
#| fig-align: center


demo_cat_preds %>% 
  unnest(.pred) %>% 
  summarize(
    weight_sum = sum(.weight_censored, na.rm = TRUE),
    `# Samples Available` = sum(!is.na(.weight_censored)),
    .by = c(.eval_time)) %>% 
  ggplot(aes(.eval_time, weight_sum, size = `# Samples Available`)) + 
  geom_point(alpha = 1 / 4) +
  labs(x = "Evaluation Time", y = "Sum of Weights") +
  theme(legend.position = "top")
```

Case weights $w_i(\tau)$ are the inverse of these probabilities and $W(\tau)$ is their sum.

## Brier Scores Over Evaluation Time

It’s reasonable to wonder about the uncertainty in these statistics, especially for a sample size of 50. When we get to resampling (in a bit), we will have standard errors of statistics that we can use to make confidence intervals. 

We can use the [bootstrap method to compute confidence intervals](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=Bootstrap+confidence+intervals+DiCiccio&btnG=) for a single data set (e.g., a validation set). A tidymodels function called `int_pctl()` is available for this purpose ([more information](https://www.tidymodels.org/learn/#category=bootstrapping)). This will be able to work directly with objects resulting form resampling or tuning functions (again, in a bit) but for a single data set, we’ll need to create a wrapper that computes the statistic for different bootstrap samples.

```{r}
#| label: brier-wrapper
brier_wrapper <- function(split) {
  dat <- analysis(split)
  brier_survival(dat, truth = event_time, .pred) %>% 
    # Puts the data into a 'tidy' format:
    dplyr::select(term = .eval_time, estimate = .estimate)
}
```

We execute that on every bootstrap sample and then use `int_pctl()` on the results. 

```{r}
#| label: brier-ints
#| cache: true

set.seed(482)
brier_ci <- 
  demo_cat_preds %>% 
  bootstraps(times = 2000) %>% 
  mutate(stats = map(splits, brier_wrapper)) %>% 
  int_pctl(stats, alpha = 0.10) # 90% intervals
```

Yep, statistics from 50 cats has a lot of variation! 

```{r}
#| label: plot-brier-ci
#| fig-width: 6
#| fig-height: 4.25
#| out-width: 80%
#| fig-align: center

brier_ci %>% 
  ggplot(aes(term, .estimate)) + 
  geom_line() + 
  geom_ribbon(
    aes(ymin = .lower, ymax = .upper), 
    alpha = 1 / 10, 
    fill = "blue") +
  labs(x = "Evaluation time", y = "Brier score")
```

## Cross-validation

In the future, we might enable the `strata` argument of the resampling functions to accept `Surv` objects and stratify by the censoring indicator. 

## Bootstrapping

[Davison and Hinkley (1997)](https://doi.org/10.1017/CBO9780511802843) have specific methods for bootstrapping censored data. These are not currently implemented in the rsample package.  

## Evaluating model performance

Note that there is a column for `std_err` so that we can compute confidence intervals from these. 

# 06 - Tuning Hyperparameters

## Update parameter ranges

In about 90% of the cases, the dials function that you use to update the parameter range has the same name as the argument. For example, if you were to update the `mtry` parameter in a random forests model, the code would look like

```{r}
#| eval: false

parameter_object %>% 
  update(mtry = mtry(c(1, 100)))
```

There are some cases where the parameter function, or its associated values, are different from the argument name. 

For example, with `step_spline_naturall()`, we might want to tune the `deg_free` argument (for the degrees of freedom of a spline function. ). In this case, the argument name is `deg_free` but we update it with `spline_degree()`. 

`deg_free` represents the general concept of degrees of freedom and could be associated with many different things. For example, if we ever had an argument that was the number of degrees of freedom for a $t$ distribution, we would call that argument `deg_free`. 

For splines, we probably want a wider range for the degrees of freedom. We made a specialized function called `spline_degree()` to be used in these cases. 

How can you tell when this happens? There is a helper function called `tunable()` and that gives information on how we make the default ranges for parameters. There is a column in these objects names `call_info`:

```{r}
library(tidymodels)
ns_tunable <- 
  recipe(mpg ~ ., data = mtcars) %>% 
  step_spline_natural(dis, deg_free = tune()) %>% 
  tunable()

ns_tunable
ns_tunable$call_info
```


## Early stopping for boosted trees

When deciding on the number of boosting iterations, there are two main strategies:

 * Directly tune it (`trees = tune()`)
 
 * Set it to one value and tune the number of early stopping iterations (`trees = 500`, `stop_iter = tune()`).

Early stopping is when we monitor the performance of the model. If the model doesn't make any improvements for `stop_iter` iterations, training stops. 

Here's an example where, after eleven iterations, performance starts to get worse. 


```{r early-stopping}
#| echo: false

roc_vals <- c(0.4994, 0.5075, 0.6131, 0.7077, 0.7376, 0.752, 0.7807, 0.7927, 
              0.7969, 0.7915, 0.8118, 0.8019, 0.7926, 0.7772, 0.776)
iterations <- seq_along(roc_vals)

early_stop <- tibble(iterations = iterations, `ROC AUC` = roc_vals)

early_stop %>% ggplot(aes(iterations, `ROC AUC`)) + geom_point() + theme_bw()
```

This is likely due to over-fitting so we stop the model at eleven boosting iterations.  

Early stopping usually has good results and takes far less time. 

We _could_ an engine argument called `validation` here. That's not an argument to any function in the lightgbm package. 

bonsai has its own wrapper around (`lightgbm::lgb.train()`) called `bonsai::train_lightgbm()`. We use that here and it has a `validation` argument.

How would you know that? There are a few different ways:

 * Look at the documentation in `?boost_tree` and click on the `lightgbm` entry in the engine list. 
 * Check out the pkgdown reference website <https://parsnip.tidymodels.org/reference/index.html>
 * Run the `translate()` function on the parsnip specification object. 

The first two options are best since they tell you a lot more about the particularities of each model engine (there are a lot for lightgbm). 
