---
title: "4 - Evaluating models - Classwork"
subtitle: "Survival analysis with tidymodels"
editor_options: 
  chunk_output_type: console
---

We recommend restarting R between each slide deck!

## Setup

Setup from deck 3

```{r}
library(tidymodels)
library(censored)
library(splines)

data(cat_adoption)

cat_adoption <- 
  cat_adoption %>% 
  mutate(event_time = Surv(time, event), .keep = "unused", .before = everything()) 

set.seed(27)
in_demo <- sample.int(nrow(cat_adoption), 50)
demo_cats <- cat_adoption %>% slice(in_demo)

set.seed(123)
cat_split <- initial_split(cat_adoption %>% slice(-in_demo), prop = 0.8)
cat_train <- training(cat_split)
cat_test <- testing(cat_split)
```

## An example model fit

```{r}
#| label: example-fit
# First add all of the predictors...
f <- event_time ~ . -
  # Then remove geocoded columns
  longitude - latitude +
  # Then add them back as spline terms
  ns(longitude, df = 5) + ns(latitude, df = 5)

cat_wflow <- workflow(f, proportional_hazards())

cph_spline_fit <- cat_wflow %>%
  fit(data = cat_train)
```

```{r}
demo_cat_preds <- augment(cph_spline_fit, demo_cats, eval_time = 8:320)
```

## Concordance

```{r}
demo_cat_preds %>% 
  concordance_survival(event_time, estimate = .pred_time)
```

## Brier Scores

```{r}
demo_brier <- brier_survival(demo_cat_preds, truth = event_time, .pred)
demo_brier %>% filter(.eval_time %in% seq(30, 300, by = 30))
```

## Integrated Brier Score

```{r}
brier_survival_integrated(demo_cat_preds, truth = event_time, .pred)
```

## Area Under the ROC Curve 

```{r}
demo_roc_auc <- roc_auc_survival(demo_cat_preds, truth = event_time, .pred)
demo_roc_auc %>% filter(.eval_time %in% seq(30, 300, by = 30))
```

## Your turn

Compute and plot an ROC curve for your current model.

What data are being used for this ROC curve plot?

```{r}
# Your code here!

```

## Dangers of overfitting

## Your turn

Why haven't we done this?

```
cph_spline_fit %>%
  augment(cat_train, eval_time = 8:320) %>%
  brier_survival(truth = event_time, .pred)
```

## Resampling

```{r}
# v = 10 is the default
vfold_cv(cat_train)
```

What is in a resampling result?

```{r}
cat_folds <- vfold_cv(cat_train)

# Individual splits of analysis/assessment data
cat_folds$splits[1:3]
```

Bootstrapping 

```{r bootstraps}
set.seed(3214)
bootstraps(cat_train)
```

We'll use this setup:

```{r}
set.seed(123)
cat_folds <- vfold_cv(cat_train, v = 10)
cat_folds
```

## Evaluating model performance

```{r}
# Fit the workflow on each analysis set,
# then compute performance on each assessment set
cat_res <- fit_resamples(cat_wflow, cat_folds, eval_time = c(90, 30, 60, 120))
cat_res
```

Aggregate metrics

```{r}
cat_res %>%
  collect_metrics()
```

If you want to analyze the assessment set (i.e. holdout) predictions, then you need to adjust the control object and tell it to save them:

```{r}
# Save the assessment set results
ctrl_cat <- control_resamples(save_pred = TRUE)

cat_res <- fit_resamples(cat_wflow, cat_folds, eval_time = c(90, 30, 60, 120),
                         control = ctrl_cat)

cat_preds <- collect_predictions(cat_res)
cat_preds
```

## Create a random forest model

```{r}
rf_spec <- rand_forest(trees = 1000) %>% 
  set_engine("aorsf") %>% 
  set_mode("censored regression")
rf_spec
```

```{r}
rf_wflow <- workflow(event_time ~ ., rf_spec)
rf_wflow
```

## Your turn

Use `fit_resamples()` and `rf_wflow` to:

- Keep predictions
- Compute metrics

```{r}
# Your code here!

```

## The final fit

```{r}
# cat_split has train + test info
final_fit <- last_fit(rf_wflow, cat_split, eval_time = c(90, 30, 60, 120)) 

final_fit
```

Test set metrics:

```{r}
collect_metrics(final_fit)
```

Use this for prediction on new data, like for deploying:

```{r}
extract_workflow(final_fit)
```
